{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Notebook_base_models.ipynb","provenance":[{"file_id":"12-mOSUZuEsgeAkOFAWXM_IiHrxITLhks","timestamp":1589735473582}],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"1rmI2rnr5e48","colab_type":"text"},"source":["# Setup"]},{"cell_type":"markdown","metadata":{"id":"H586gtoqJ0cu","colab_type":"text"},"source":["## Imports "]},{"cell_type":"code","metadata":{"id":"_2X1OcpKmO35","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Hu65BTA5ikD","colab_type":"code","colab":{}},"source":["from PIL import Image\n","from sklearn import utils\n","from sklearn.model_selection import ParameterGrid\n","from sklearn.model_selection import train_test_split\n","from torch.autograd import Function\n","from torch.backends import cudnn\n","from torch.utils.data import DataLoader\n","from torch.utils.data import Subset, DataLoader\n","from torch.utils.model_zoo import load_url as load_state_dict_from_url\n","from torchvision import models\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torchvision.datasets import VisionDataset\n","from torchvision.models import alexnet\n","from torchvision.transforms.functional import pad\n","from tqdm import tqdm\n","import logging\n","import matplotlib.pyplot as plt\n","import numbers\n","import numpy as np\n","import os\n","import os.path\n","import shutil\n","import sys\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","import zipfile\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WTBUvYrKj5Q2","colab_type":"text"},"source":["## Download datasets in colab\n","Always working for everyone"]},{"cell_type":"code","metadata":{"id":"lvMQqD8Wj4Vi","colab_type":"code","colab":{}},"source":["# Download ROD dataset\n","rod_destination_path = \"/content/ROD\"\n","if not os.path.isdir(rod_destination_path):\n","  # ROD \n","  # https://drive.google.com/open?id=1p1GORdB44NjtNWJ4d1xqttseM1X9lWNF\n","  # https://drive.google.com/open?id=168neCvaHwMffFOqjOkth-wVaP4tRFuSW\n","  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1p1GORdB44NjtNWJ4d1xqttseM1X9lWNF\" > /dev/null\n","  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1p1GORdB44NjtNWJ4d1xqttseM1X9lWNF\" -o \"ROD.zip\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WKRdgvgGkJ4f","colab_type":"code","colab":{}},"source":["# Download synROD dataset\n","synrod_destination_path = \"/content/synROD\"\n","if not os.path.isdir(synrod_destination_path):\n","  # synROD \n","  # https://drive.google.com/open?id=1rry4GViJLmmMpbm0B2s7MyQs5Dx8pFS3\n","  # https://drive.google.com/open?id=1V1fthSNAvsPRF6hLt_kf_xonw7lxAV03\n","  !curl -c ./cookie -s -L \"https://drive.google.com/uc?export=download&id=1rry4GViJLmmMpbm0B2s7MyQs5Dx8pFS3\" > /dev/null\n","  !curl -Lb ./cookie \"https://drive.google.com/uc?export=download&confirm=`awk '/download/ {print $NF}' ./cookie`&id=1rry4GViJLmmMpbm0B2s7MyQs5Dx8pFS3\" -o \"synROD.zip\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3f6vZnCkQuk","colab_type":"code","colab":{}},"source":["# Extract ROD dataset\n","rod_destination_path = \"/content/ROD\"\n","if not os.path.isdir(rod_destination_path):\n","  with zipfile.ZipFile(\"/content/ROD.zip\", 'r') as zip_ref:\n","      zip_ref.extractall(rod_destination_path)\n","!rm ROD.zip\n","\n","# Extract synROD dataset\n","synrod_destination_path = \"/content/synROD\"\n","if not os.path.isdir(synrod_destination_path):\n","  with zipfile.ZipFile(\"/content/synROD.zip\", 'r') as zip_ref:\n","      zip_ref.extractall(synrod_destination_path)\n","!rm synROD.zip\n","!rm cookie\n","\n","rod_path = \"/content/ROD/ROD\"\n","synrod_path = \"/content/synROD/synROD\"\n","\n","\n","if os.path.isdir(os.path.join(synrod_path,  \"bell_papper\")):\n","  # line below needed only for the first extraction of the synrod dataset, in which bell_pepper is wrongly named bell_papper\n","  os.rename(os.path.join(synrod_path,  \"bell_papper\"), os.path.join(synrod_path,  \"bell_pepper\") )\n","  print(\"Bell pepper fixed\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eWXTNv9OJsnr","colab_type":"text"},"source":["## Utility functions"]},{"cell_type":"code","metadata":{"id":"EF4lKNlJJvAy","colab_type":"code","colab":{}},"source":["class HLoss(nn.Module):\n","    def __init__(self):\n","        super(HLoss, self).__init__()\n","\n","    def forward(self, x):\n","        b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n","        b = -1.0 * b.sum()\n","        return b\n","\n","# DATALOADER function\n","def collate(batch):\n","  return batch\n","\n","#default init weights\n","def init_weights(m):\n","    if type(m) == nn.Conv2d or type(m) == nn.Linear :\n","        nn.init.xavier_uniform_(m.weight)\n","        nn.init.zeros_(m.bias)\n","\n","# helper function used to setup batches returned by the dataloaders in the way that is mentioned in the paper\n","def format_batch(batch, pretext_task=\"rotation\"):\n","  \"\"\"\"\n","  set pretext_task == rotation to require the pretext task labels to be of type \"long\"\n","  set pretext_task == zoom to require the pretext task labels to be of type \"float\"\n","  \"\"\"\n","  data = {\"rgb\":[], \"depth\":[], \"label\":[] }\n","  data_hat = {\"rgb\":[], \"depth\":[], \"label\":[] }\n","  for tuple_, tuple_hat in batch:\n","    rgb_img, depth_img, label = tuple_\n","    rot_rgb_img, rot_depth_img, rot_label = tuple_hat\n","\n","    data[\"rgb\"].append(rgb_img[None,:])\n","    data[\"depth\"].append(depth_img[None,:])\n","    data[\"label\"].append(label)\n","\n","    data_hat[\"rgb\"].append(rot_rgb_img[None,:])\n","    data_hat[\"depth\"].append(rot_depth_img[None,:])\n","    data_hat[\"label\"].append(rot_label)\n","  \n","  data[\"rgb\"] = torch.cat(data[\"rgb\"] , dim=0) \n","  data[\"depth\"] = torch.cat(data[\"depth\"] , dim=0)\n","  data[\"label\"] = torch.LongTensor(data[\"label\"])\n","  \n","  data_hat[\"rgb\"] = torch.cat(data_hat[\"rgb\"] , dim=0) \n","  data_hat[\"depth\"] = torch.cat(data_hat[\"depth\"] , dim=0)\n","  if pretext_task == \"rotation\":\n","    data_hat[\"label\"] = torch.LongTensor(data_hat[\"label\"] )\n","  else:\n","    data_hat[\"label\"] = torch.FloatTensor(data_hat[\"label\"] )\n","  \n","  return data, data_hat\n","\n","\n","#side by side loss and accuracy plot\n","def make_plot(train_loss, train_acc, test_loss, test_acc):\n","  f = plt.figure(figsize=(10,3))\n","  ax1 = f.add_subplot(121)\n","  ax2 = f.add_subplot(122)\n","\n","  # plot all points registered during training\n","  ax1.plot(range(0,len(train_loss)), train_loss, label=\"train\")\n","  ax1.plot(range(0, len(test_loss)), test_loss, label=\"test\")\n","\n","  # or average them for each epoch and plot per epoch\n","  #ax1.plot(range(0, num_epochs), train_loss, label=\"train\")\n","  #ax1.plot(range(0, num_epochs), test_loss, label=\"test\")\n","  ax1.set_title(\"loss\")\n","  ax1.grid()\n","  #uncomment this to decide scale of the loss plot\n","  #ax1.set_ylim(0, 5)\n","  ax1.legend()\n","  ax2.plot(range(0, len(train_acc)),train_acc , label=\"train\")\n","  ax2.plot(range(0, len(test_acc)), test_acc, label=\"test\")\n","\n","  #ax2.plot(range(0, num_epochs),train_acc , label=\"train\")\n","  #ax2.plot(range(0, num_epochs), test_acc, label=\"test\")\n","  ax2.set_title(\"accuracy\")\n","  #uncomment this to decide scale of the accuracy plot\n","  #ax2.set_ylim(0,1.05)\n","  ax2.grid()\n","  ax2.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"9O8uN5WxTOEm","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","def learning_curves(training_accuracies, training_losses, validation_accuracies, validation_losses, plot_title, plot_size=(16,6)):\n","  \"\"\"\n","  Plots accuracies and losses per epochs.\n","  \"\"\"\n","  fig, ax = plt.subplots(nrows=1, ncols=2, figsize=plot_size)\n","  ax[0].plot(range(1,len(training_accuracies)+1), training_accuracies, label=\"Training\")\n","  ax[0].plot(range(1,len(validation_accuracies)+1), validation_accuracies, label=\"Validation\")\n","  ax[0].legend()\n","  ax[0].set_title(\"Accuracy\")\n","  ax[0].set_xlabel(\"Epochs\")\n","\n","  ax[1].plot(range(1,len(training_losses)+1), training_losses, label=\"Training\")\n","  ax[1].plot(range(1,len(validation_losses)+1), validation_losses, label=\"Validation\")\n","  ax[1].legend()\n","  ax[1].set_title(\"Loss\")\n","  ax[1].set_xlabel(\"Epochs\")\n","\n","  fig.suptitle(plot_title)\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CRk1caT8K31v","colab_type":"text"},"source":["## Copy in current folder datasets and net classes"]},{"cell_type":"code","metadata":{"id":"4B__Yny4K9o3","colab_type":"code","colab":{}},"source":["!cp -r \"/content/drive/My Drive/DL_project/dataset/.\" \"/content/\"\n","!cp -r \"/content/drive/My Drive/DL_project/net/.\" \"/content/\"\n","!cp -r \"/content/drive/My Drive/DL_project/transform_config/.\" \"/content/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sp9oWROBKxqm","colab_type":"text"},"source":["## Import datasets, net and configurator classes"]},{"cell_type":"code","metadata":{"id":"IEaktSSiK2L2","colab_type":"code","colab":{}},"source":["from synrod import SynROD\n","from rod import ROD\n","\n","from dnet import DNet\n","\n","from tconfig import TransformConfig"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ivOnu9RQ2Miz","colab_type":"text"},"source":["# Train test function definitions"]},{"cell_type":"markdown","metadata":{"id":"PDM_VvWPLgY1","colab_type":"text"},"source":["## Train test RGB/ DEPTH ONLY function definition"]},{"cell_type":"code","metadata":{"id":"0ReCuXRtLkaW","colab_type":"code","colab":{}},"source":["def train_net(synrod, rod, hyperparams, type=\"rgb\", light_validation=False):\n","  \"\"\"Train the resnet18 in rgb or depth mode.  \n","\n","  Args:\n","    synrod: train dataset\n","    rod: test dataset\n","    type: \"rgb\" or \"depth\"\n","    hyperparams: parameters dict with the keys\n","      {\n","        lr\n","        batch_size\n","        weight_decay\n","        step_size\n","        epochs\n","        momentum (optional, default 0.9)\n","        gamma (optional, default 0.1)\n","      }\n","    light_validation: if True the validation is done only in the last epoch.\n","\n","  Return: \n","    (trained_model, train_loss, train_acc, test_loss, test_acc).\n","  \"\"\"\n","  lr = hyperparams[\"lr\"]\n","  batch_size = hyperparams[\"batch_size\"]\n","  weight_decay = hyperparams[\"weight_decay\"]\n","  step_size = hyperparams[\"step_size\"]\n","  epochs = hyperparams[\"epochs\"]\n","  curr_momentum = hyperparams.get(\"momentum\", 0.9)\n","  curr_gamma = hyperparams.get(\"gamma\", 0.1) \n"," \n","  DEVICE = \"cuda\"\n","  cudnn.benchmark\n","\n","  # dataloader definition with given batch size\n","  source = DataLoader(synrod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","  target = DataLoader(rod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","  \n","  # NET DEFINITION\n","  model = models.resnet18(pretrained=True)\n","  outLayer = torch.nn.Linear(512, 51) \n","  nn.init.xavier_uniform_(outLayer.weight)\n","  nn.init.zeros_(m.bias)\n","  model.fc = outLayer\n","  model = model.to(DEVICE)\n","\n","  criterion = nn.CrossEntropyLoss() \n","\n","  optimizer = optim.SGD(model.parameters(), lr=lr, momentum=curr_momentum, weight_decay=weight_decay)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=curr_gamma)\n","\n","  train_loss = []\n","  train_acc = []\n","  test_loss = []\n","  test_acc = []\n","\n","  train_running_corrects = 0\n","  test_running_corrects = 0\n","\n","  # TRAIN NET\n","  for i in range(epochs):\n","    n_iters = 0\n","    train_acc_val = 0\n","    train_loss_val = 0\n","    test_acc_val = 0\n","    test_loss_val = 0\n","    \n","    for source_batch in source:\n","      # Train mode and reset accumulated gradients\n","      model.train()\n","      optimizer.zero_grad()\n","      \n","      S, _ = format_batch(source_batch)\n","\n","      # Prepare data\n","      source_main_labels = S[\"label\"].to(DEVICE)\n","      \n","      #TRAINING SECTION\n","      if type == \"rgb\":\n","        source_rgb_images = S[\"rgb\"].to(DEVICE)\n","        outputs = model(source_rgb_images)\n","      elif type == \"depth\":\n","        source_depth_images = S[\"depth\"].to(DEVICE)\n","        outputs = model(source_depth_images)\n","      else:\n","        return None\n","      loss = criterion(outputs, source_main_labels)\n","      _, preds = torch.max(outputs.data, 1)\n","      train_running_corrects = torch.sum(preds == source_main_labels.data).data.item()\n","      loss.backward() \n","      optimizer.step()\n","      \n","      # cumulatives of current epoch\n","      train_acc_val += train_running_corrects/len(source_main_labels)\n","      train_loss_val += loss.item()\n","\n","      n_iters += 1\n","\n","    train_acc.append(train_acc_val/n_iters)\n","    train_loss.append(train_loss_val/n_iters)\n","\n","    print(\"EPOCH \", i + 1)\n","    print(\"train accuracy: \", train_acc_val/n_iters)\n","    print(\"train loss: \", train_loss_val/n_iters)\n","\n","\n","\n","    #VALIDATION SECTION\n","    if not light_validation or i == epochs - 1:\n","      model.eval()\n","      n_iters = 0\n","      for target_batch in target:\n","        \n","        T, _ = format_batch(target_batch)\n","\n","        target_main_labels = T[\"label\"].to(DEVICE)\n","\n","        if type == \"rgb\":\n","          target_rgb_images = T[\"rgb\"].to(DEVICE)\n","          outputs = model(target_rgb_images)\n","        elif type == \"depth\":\n","          target_depth_images = T[\"depth\"].to(DEVICE)\n","          outputs = model(target_depth_images)\n","\n","        loss = criterion(outputs, target_main_labels)\n","        _, preds = torch.max(outputs.data, 1)\n","        test_running_corrects = torch.sum(preds == target_main_labels.data).data.item()\n","\n","        test_acc_val += test_running_corrects/len(target_main_labels)\n","        test_loss_val += loss.item()\n","\n","        n_iters += 1\n","\n","      test_acc.append(test_acc_val/n_iters)\n","      test_loss.append(test_loss_val/n_iters)\n","        \n","      print(\"test accuracy: \", test_acc_val/n_iters)\n","      print(\"test loss: \", test_loss_val/n_iters)\n","    \n","    print()\n","    scheduler.step()\n","\n","  return model, train_loss, train_acc, test_loss, test_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5oKkrNmSLsQ_","colab_type":"text"},"source":["## Train test RGB-D FROZEN function definition"]},{"cell_type":"code","metadata":{"id":"VdDwlmSjMEEi","colab_type":"code","colab":{}},"source":["def train_rgbd_net(synrod, rod, hyperparams, trained_RGB_net=None, trained_DEPTH_net=None, light_validation=False):\n","  \"\"\"Merges together two features extractors (resnet18) ptretrained for rgb and depth modalities.\n","  Features extractors are frozen and is added a xavier initialized fully connected layer on top of them.\n","\n","  Args:\n","    synrod: train dataset\n","    rod: test dataset\n","    hyperparams: parameters dict with the keys\n","      {\n","        lr\n","        batch_size\n","        weight_decay \n","        step_size\n","        epochs \n","        momentum (optional, default 0.9)\n","        gamma (optional, default 0.1)\n","      }\n","    trained_RGB_net: Pretrained resnet18 on RGB only images. Should be specified\n","    trained_DEPTH_net: Pretrained resnet18 on depth only images. Should be specified\n","    light_validation: if True the validation is done only in the last epoch.\n","\n","  Return: \n","    (trained_model, train_loss, train_acc, test_loss, test_acc).\n","  \"\"\"\n","  lr = hyperparams[\"lr\"]\n","  batch_size = hyperparams[\"batch_size\"]\n","  weight_decay = hyperparams[\"weight_decay\"]\n","  step_size = hyperparams[\"step_size\"]\n","  epochs = hyperparams[\"epochs\"]\n","  curr_momentum = hyperparams.get(\"momentum\", 0.9)\n","  curr_gamma = hyperparams.get(\"gamma\", 0.1) \n","  \n","  DEVICE = \"cuda\"\n","  cudnn.benchmark\n","\n","  # dataloader definition with given batch size\n","  source = DataLoader(synrod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","  target = DataLoader(rod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","  \n","  # Feature extractors\n","  if trained_RGB_net is None:\n","    print(\"##### Start training RGB feature extractor #####\")\n","    trained_RGB_net, _, _, _, _ = train_net(synrod, rod, hyperparams, type=\"rgb\", light_validation=True)\n","    print(\"##### End training RGB feature extractor #####\")\n","    print()\n","    print()\n","\n","  if trained_DEPTH_net is None:\n","    print(\"##### Start training DEPTH feature extractor #####\")\n","    trained_DEPTH_net, _, _, _, _ = train_net(synrod, rod, hyperparams, type=\"depth\", light_validation=True)\n","    print(\"##### End training DEPTH feature extractor #####\")\n","    print()\n","    print()\n"," \n","  net = DNet(num_classes=51, resnet1=trained_RGB_net, resnet2=trained_RGB_net)\n","  net.Pbranch = None\n","  \n","  net.Mbranch = nn.Sequential(\n","              nn.AdaptiveAvgPool2d((1,1)),\n","              nn.Flatten(),\n","              nn.Linear(1024, 51)\n","          )\n","  net = net.to(DEVICE)\n","\n","  # to avoid that the backward pass wastes time calculating gradients for parameters that will\n","  # not be update it is necessary to set the flag to false\n","  for param in trained_RGB_net.parameters():\n","    param.requires_grad = False\n","  for param in trained_DEPTH_net.parameters():\n","    param.requires_grad = False\n","\n","  # final fully connected branch that is trained after the two feature extractors are trained and frozen. \n","  # The full net consists of model_RGB + model_DEPTH, both frozen while fusionBranch is trained \n","\n","  criterion = nn.CrossEntropyLoss() \n","\n","  optimizer = optim.SGD(net.Mbranch.parameters(), lr=lr, momentum=curr_momentum, weight_decay=weight_decay)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=curr_gamma)\n","\n","  train_loss = []\n","  train_acc = []\n","  test_loss = []\n","  test_acc = []\n","\n","  running_corrects = 0\n","  \n","  # TRAIN FUSION NETWORK\n","  for i in range(epochs):\n","    n_iters = 0\n","    train_acc_val = 0\n","    train_loss_val = 0\n","    test_acc_val = 0\n","    test_loss_val = 0\n","    \n","    net.train()\n","    for source_batch in source:\n","      S, _ = format_batch(source_batch, pretext_task=\"rotation\")\n","\n","      # Prepare data\n","      source_rgb_images = S[\"rgb\"].to(DEVICE)\n","      source_depth_images = S[\"depth\"].to(DEVICE)\n","      source_main_labels = S[\"label\"].to(DEVICE)\n","      \n","      # FUSION ONLY TRAINING SECTION\n","      optimizer.zero_grad()\n","      outputs = net.forward(source_rgb_images, source_depth_images, mode=\"main\", debug=False)\n","      loss = criterion(outputs, source_main_labels)\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_main_labels.data).data.item()\n","      loss.backward() \n","      optimizer.step()\n","      \n","      # log results obtained by training the fusion net over the current batch\n","      train_acc_val += running_corrects/len(source_main_labels)\n","      train_loss_val += loss.item()\n","\n","      n_iters += 1\n","\n","    print(\"EPOCH \", i + 1)\n","    print(\"train FUSION accuracy: \", train_acc_val/n_iters)\n","    print(\"train FUSION loss: \", train_loss_val/n_iters)\n","    train_loss.append(train_loss_val/n_iters)\n","    train_acc.append(train_acc_val/n_iters)\n","\n","    #VALIDATION SECTION\n","    if not light_validation or i == epochs - 1:\n","      n_iters = 0\n","      net.eval()\n","      for target_batch in target:\n","        T, _ = format_batch(target_batch, pretext_task=\"rotation\")\n","        target_rgb_images = T[\"rgb\"].to(DEVICE)\n","        target_depth_images = T[\"depth\"].to(DEVICE)\n","        target_main_labels = T[\"label\"].to(DEVICE)\n","\n","        outputs = net.forward(target_rgb_images, target_depth_images, mode=\"main\", debug=False)\n","\n","        loss_T_FUSION = criterion(outputs, target_main_labels)\n","        _, preds = torch.max(outputs.data, 1)\n","        running_corrects = torch.sum(preds == target_main_labels.data).data.item()\n","\n","        test_acc_val += running_corrects/len(target_main_labels)\n","        test_loss_val += loss_T_FUSION.item()\n","\n","        n_iters += 1\n","      \n","      test_loss.append(test_loss_val/n_iters)\n","      test_acc.append(test_acc_val/n_iters)\n","\n","      print(\"test target FUSION accuracy: \", test_acc_val/n_iters)\n","      print(\"test target FUSION loss: \", test_loss_val/n_iters)\n","\n","    print()\n","    scheduler.step()\n","\n","  return net, train_loss, train_acc, test_loss, test_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TkXQIunvPxRY","colab_type":"text"},"source":["## Train test RGB-D E2E function definition"]},{"cell_type":"code","metadata":{"id":"7qcLcs3yP0nl","colab_type":"code","colab":{}},"source":["def train_rgbde2e_net(synrod, rod, hyperparams, light_validation=False):\n","  \"\"\"Merges together two features extractors (resnet18) for rgb and depth modalities.\n","  A xavier initialized fully connected layer is added on top of them.\n","  The net is trained in end-to-end fashion.\n","\n","  Args:\n","    synrod: train dataset\n","    rod: test dataset\n","    hyperparams: parameters dict with the keys\n","      {\n","        lr\n","        batch_size\n","        weight_decay \n","        step_size\n","        epochs \n","        momentum (optional, default 0.9)\n","        gamma (optional, default 0.1)\n","      }\n","    light_validation: if True the validation is done only in the last epoch.\n","\n","  Return: \n","    (trained_model, train_loss, train_acc, test_loss, test_acc).\n","  \"\"\"\n","  lr = hyperparams[\"lr\"]\n","  batch_size = hyperparams[\"batch_size\"]\n","  weight_decay = hyperparams[\"weight_decay\"]\n","  step_size = hyperparams[\"step_size\"]\n","  epochs = hyperparams[\"epochs\"]\n","  curr_momentum = hyperparams.get(\"momentum\", 0.9)\n","  curr_gamma = hyperparams.get(\"gamma\", 0.1) \n","\n","  DEVICE = \"cuda\"\n","  cudnn.benchmark\n","\n","  # dataloader definition with given batch size\n","  source = DataLoader(synrod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","  target = DataLoader(rod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","  \n","  # NET DEFINITION\n","  net = DNet(num_classes=51)\n","  net.Pbranch = None\n","\n","  #uncomment this section to train model as descripted in paper (RGB-De2e)\n","  #net.Mbranch = nn.Sequential(\n","              #nn.AdaptiveAvgPool2d((1,1)),\n","              #nn.Flatten(),\n","              #nn.Linear(1024, 51)\n","          #)\n","  #net.Mbranch.apply(init_weights)\n","  net = net.to(DEVICE)\n","\n","  criterion = nn.CrossEntropyLoss() \n","\n","  #parameters_to_optimize = list(fusionBranch.parameters()) + list(RGB_net.parameters()) + list(DEPTH_net.parameters()) \n","  parameters_to_optimize = net.parameters()\n","  optimizer = optim.SGD(parameters_to_optimize, lr=lr, momentum=curr_momentum, weight_decay=weight_decay)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=curr_gamma)\n","\n","  train_loss = []\n","  train_acc = []\n","  test_loss = []\n","  test_acc = []\n","\n","  running_corrects = 0\n","  \n","  # TRAIN FUSION NETWORK\n","  for i in range(epochs):\n","    n_iters = 0\n","    train_acc_val = 0\n","    train_loss_val = 0\n","    test_acc_val = 0\n","    test_loss_val = 0\n","    \n","    net.train()\n","    for source_batch in source:\n","      S, _ = format_batch(source_batch, pretext_task=\"rotation\")\n","\n","      # Prepare data\n","      source_rgb_images = S[\"rgb\"].to(DEVICE)\n","      source_depth_images = S[\"depth\"].to(DEVICE)\n","      source_main_labels = S[\"label\"].to(DEVICE)\n","      \n","      # TRAINING SECTION\n","      optimizer.zero_grad()\n","      outputs = net.forward(source_rgb_images, source_depth_images, mode=\"main\", debug=False)\n","      loss = criterion(outputs, source_main_labels)\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_main_labels.data).data.item()\n","      loss.backward() \n","      optimizer.step()\n","      \n","      # log results obtained by training the fusion net over the current batch\n","      train_acc_val += running_corrects/len(source_main_labels)\n","      train_loss_val += loss.item()\n","\n","      n_iters += 1\n","\n","    print(\"EPOCH \", i + 1)\n","    print(\"train e2e accuracy: \", train_acc_val/n_iters)\n","    print(\"train e2e loss: \", train_loss_val/n_iters)\n","    train_loss.append(train_loss_val/n_iters)\n","    train_acc.append(train_acc_val/n_iters)\n","\n","    #VALIDATION SECTION\n","    if not light_validation or i == epochs - 1:\n","      n_iters = 0\n","      net.eval()\n","      for target_batch in target:\n","        T, _ = format_batch(target_batch, pretext_task=\"rotation\")\n","        target_rgb_images = T[\"rgb\"].to(DEVICE)\n","        target_depth_images = T[\"depth\"].to(DEVICE)\n","        target_main_labels = T[\"label\"].to(DEVICE)\n","\n","        outputs = net.forward(target_rgb_images, target_depth_images, mode=\"main\", debug=False)\n","\n","        loss_T_FUSION = criterion(outputs, target_main_labels)\n","        _, preds = torch.max(outputs.data, 1)\n","        running_corrects = torch.sum(preds == target_main_labels.data).data.item()\n","\n","        test_acc_val += running_corrects/len(target_main_labels)\n","        test_loss_val += loss_T_FUSION.item()\n","\n","        n_iters += 1\n","      \n","      test_loss.append(test_loss_val/n_iters)\n","      test_acc.append(test_acc_val/n_iters)\n","\n","      print(\"test target e2e accuracy: \", test_acc_val/n_iters)\n","      print(\"test target e2e loss: \", test_loss_val/n_iters)\n","\n","    print()\n","    scheduler.step()\n","\n","  return net, train_loss, train_acc, test_loss, test_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1uvqfWHZQ2mH","colab_type":"text"},"source":["## Train test OURS function definition"]},{"cell_type":"code","metadata":{"id":"P57r29_XQ5M9","colab_type":"code","colab":{}},"source":["def train_test_ours(synrod, rod, hyperparams, light_validation=False):\n","  \"\"\"Train the architecture called \"OURS\" in the reference paper with rotation pretext task.\n","  The net is trained in end-to-end fashion.\n","\n","  Args:\n","    synrod: train dataset\n","    rod: test dataset\n","    hyperparams: parameters dict with the keys\n","      {\n","        lr\n","        batch_size\n","        weight_decay \n","        step_size\n","        epochs \n","        lambda (!!!)\n","        momentum (optional, default 0.9)\n","        gamma (optional, default 0.1)\n","      }\n","    light_validation: if True the validation is done only in the last epoch.\n","\n","  Return: \n","    (trained_model, train_loss, train_acc, test_loss, test_acc).\n","  \"\"\"\n","  lr = hyperparams[\"lr\"]\n","  batch_size = hyperparams[\"batch_size\"]\n","  weight_decay = hyperparams[\"weight_decay\"]\n","  step_size = hyperparams[\"step_size\"]\n","  epochs = hyperparams[\"epochs\"]\n","  curr_momentum = hyperparams.get(\"momentum\", 0.9)\n","  curr_gamma = hyperparams.get(\"gamma\", 0.1) \n","  lambda_ = hyperparams[\"lambda\"]\n","  em_weight = 0.1\n","  \n","  DEVICE = \"cuda\"\n","  cudnn.benchmark\n","\n","  # dataloader definition with given batch size\n","  source = DataLoader(synrod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","  source_rot = DataLoader(synrod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","  target_rot = DataLoader(rod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","  target = DataLoader(rod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=4, collate_fn=collate)\n","\n","  # NET DEFINITION\n","  net = DNet(num_classes=51, dim_pretext=4).to(DEVICE)\n","\n","  criterion = nn.CrossEntropyLoss() \n","  entropy_min_criterion = HLoss()\n","\n","  parameters_to_optimize = net.parameters() \n","  optimizer = optim.SGD(parameters_to_optimize, lr=lr, \n","                            momentum=curr_momentum, \n","                            weight_decay=weight_decay)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=curr_gamma)\n","\n","  # lists that accumulate loss/accuracy values over the training period\n","  train_loss = []\n","  train_acc = []\n","  test_loss = []\n","  test_acc = []\n","  # lambda parameter used to weight the losses of main and pretext task\n"," \n","  running_corrects = 0\n","  tot_samples = 0\n","  for i in range(epochs):\n","    counter_mod = 0\n","    n_iters = 0\n","    train_main_acc_val = 0\n","    train_main_loss_val = 0\n","\n","    train_rot_source_acc_val = 0\n","    train_rot_source_loss_val = 0\n","\n","    train_rot_target_acc_val = 0\n","    train_rot_target_loss_val = 0\n","    \n","    test_main_acc_val = 0\n","    test_main_loss_val = 0\n","\n","    for source_batch, target_batch, source_rot_batch, target_rot_batch in zip(source, target, source_rot, target_rot):\n","      \n","      net.train()\n","      S, _ = format_batch(source_batch, pretext_task=\"rotation\")\n","      T, _ = format_batch(target_batch, pretext_task=\"rotation\")\n","\n","      _, S_hat = format_batch(source_rot_batch, pretext_task=\"rotation\")\n","      _, T_hat = format_batch(target_rot_batch, pretext_task=\"rotation\")\n","\n","      # zero the gradients\n","      optimizer.zero_grad() \n","\n","      # MAIN TASK\n","      # setup SOURCE DOMAIN STANDARD dataset to feed to the net\n","      source_rgb_images = S[\"rgb\"].to(DEVICE)\n","      source_depth_images = S[\"depth\"].to(DEVICE)\n","      source_main_labels = S[\"label\"].to(DEVICE)\n","      \n","      # train on source original images \n","      outputs = net.forward(source_rgb_images, source_depth_images, mode=\"main\")\n","      loss_M = criterion(outputs, source_main_labels)\n","      \n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_main_labels.data).data.item()\n","      tot_samples = len(source_main_labels)\n","      train_main_loss_val += loss_M.item()\n","      train_main_acc_val += running_corrects/tot_samples\n","      \n","      # entropy minimization\n","      # setup TARGET DOMAIN STANDARD dataset to feed to the net\n","      target_rgb_images = T[\"rgb\"].to(DEVICE)\n","      target_depth_images = T[\"depth\"].to(DEVICE)\n","      # target labels in this phase can't be used for training\n","      # train on source original images \n","      outputs = net.forward(target_rgb_images, target_depth_images, mode=\"main\")\n","      loss_entropy_min = entropy_min_criterion(outputs)\n","          \n","\n","\n","      # PRETEXT TASK \n","      # setup  SOURCE DOMAIN ROTATED dataset to feed to the net\n","      source_rotated_rgb_images = S_hat[\"rgb\"].to(DEVICE)\n","      source_rotated_depth_images = S_hat[\"depth\"].to(DEVICE)\n","      source_rotated_labels = S_hat[\"label\"].to(DEVICE)\n","      \n","      # train on source rotated \n","      outputs = net.forward(source_rotated_rgb_images, source_rotated_depth_images, mode=\"pretext\")\n","      loss_P_1 = criterion(outputs, source_rotated_labels) \n","      \n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_rotated_labels.data).data.item()\n","      tot_samples = len(source_rotated_labels)\n","      train_rot_source_acc_val += running_corrects/tot_samples\n","      train_rot_source_loss_val += loss_P_1.item()\n","\n","      #setup TARGET DOMAIN ROTATED dataset to feed to the net\n","      target_rotated_rgb_images = T_hat[\"rgb\"].to(DEVICE)\n","      target_rotated_depth_images = T_hat[\"depth\"].to(DEVICE)\n","      target_rotated_labels = T_hat[\"label\"].to(DEVICE)\n","      \n","      # train on target rotated\n","      outputs = net.forward(target_rotated_rgb_images, target_rotated_depth_images, mode=\"pretext\")\n","      loss_P_2 = criterion(outputs, target_rotated_labels)\n","      \n","      # compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == target_rotated_labels.data).data.item()\n","      tot_samples = len(target_rotated_labels)\n","      train_rot_target_acc_val += running_corrects/tot_samples\n","      train_rot_target_loss_val += loss_P_2.item()\n","\n","\n","      # BACKPROP WITH THE FULL LOSS\n","      loss = loss_M + (em_weight/tot_samples)*loss_entropy_min + lambda_*(loss_P_1 + loss_P_2)\n"," \n","      loss.backward() \n","      # UPDATE GRADIENTS\n","      optimizer.step()\n","\n","      n_iters += 1\n","    \n","    train_loss.append(train_main_loss_val/n_iters)\n","    train_acc.append(train_main_acc_val/n_iters)\n","\n","    print(\"EPOCH: \", i + 1)\n","    print(\"train main accuracy: \", train_main_acc_val/n_iters)\n","    print(\"train main loss: \", train_main_loss_val/n_iters)\n","    print(\"train rot source accuracy: \", train_rot_source_acc_val/n_iters)\n","    print(\"train rot source loss: \", train_rot_source_loss_val/n_iters)\n","    print(\"train rot target accuracy: \", train_rot_target_acc_val/n_iters)\n","    print(\"train rot target loss: \", train_rot_target_loss_val/n_iters)\n","\n","    # TEST RESULTS OF THE CURRENT BATCH OF TRAINING\n","    if not light_validation or i == epochs - 1:\n","      net.eval() \n","      n_iters = 0\n","      for target_batch in target: \n","        # Format batch\n","        T, _ = format_batch(target_batch, pretext_task=\"rotation\")\n","\n","        # prepare target data\n","        target_rgb_images = T[\"rgb\"].to(DEVICE)\n","        target_depth_images = T[\"depth\"].to(DEVICE)\n","        target_main_labels = T[\"label\"].to(DEVICE)\n","\n","        outputs = net.forward(target_rgb_images, target_depth_images, mode=\"main\")\n","        loss_T = criterion(outputs, target_main_labels)\n","\n","        # compute test stats\n","        _, preds = torch.max(outputs.data, 1)\n","        running_corrects = torch.sum(preds == target_main_labels.data).data.item()\n","        test_main_acc_val += running_corrects/len(target_main_labels)\n","        test_main_loss_val += loss_T.item()\n","\n","        n_iters += 1\n","      \n","      test_loss.append(test_main_loss_val/n_iters)\n","      test_acc.append(test_main_acc_val/n_iters)\n","\n","      print(\"test main target accuracy: \", test_main_acc_val/n_iters)\n","      print(\"test main target loss: \", test_main_loss_val/n_iters)\n","\n","    print()\n","    scheduler.step()\n","  \n","  return net, train_loss, train_acc, test_loss, test_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XTSnQWEdKW-K","colab_type":"text"},"source":["## Train test VARIATION function definition"]},{"cell_type":"code","metadata":{"id":"HI0VOKnfKbkb","colab_type":"code","colab":{}},"source":["def train_test_variation(synrod, rod, hyperparams, light_validation=False):\n","  \"\"\"Train our variation to the paper architecture with zoom pretext task.\n","  The net is trained in end-to-end fashion.\n","\n","  Args:\n","    synrod: train dataset\n","    rod: test dataset\n","    hyperparams: parameters dict with the keys\n","      {\n","        lr\n","        batch_size\n","        weight_decay \n","        step_size\n","        epochs \n","        lambda (!!!)\n","        momentum (optional, default 0.9)\n","        gamma (optional, default 0.1)\n","      }\n","    light_validation: if True the validation is done only in the last epoch.\n","\n","  Return: \n","    (trained_model, train_loss, train_acc, test_loss, test_acc).\n","  \"\"\"\n","  lr = hyperparams[\"lr\"]\n","  batch_size = hyperparams[\"batch_size\"]\n","  weight_decay = hyperparams[\"weight_decay\"]\n","  step_size = hyperparams[\"step_size\"]\n","  epochs = hyperparams[\"epochs\"]\n","  curr_momentum = hyperparams.get(\"momentum\", 0.9)\n","  curr_gamma = hyperparams.get(\"gamma\", 0.1) \n","  lambda_ = hyperparams[\"lambda\"]\n","  em_weight = 0.1\n","  DEVICE = \"cuda\"\n","  cudnn.benchmark\n","\n","  # dataloader definition with given batch size\n","  N_WORKERS = 4\n","  source = DataLoader(synrod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=N_WORKERS, collate_fn=collate)\n","  source_rot = DataLoader(synrod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=N_WORKERS, collate_fn=collate)\n","  target_rot = DataLoader(rod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=N_WORKERS, collate_fn=collate)\n","  target = DataLoader(rod,  batch_size=batch_size, shuffle=True, drop_last=True, num_workers=N_WORKERS, collate_fn=collate)\n","\n","  # NET DEFINITION\n","  net = DNet(num_classes=51, dim_pretext=1)\n","  net = net.to(DEVICE)\n","\n","  main_criterion = nn.CrossEntropyLoss() \n","  pretext_criterion = nn.MSELoss() \n","  entropy_min_criterion = HLoss()\n","\n","  parameters_to_optimize = net.parameters() \n","  optimizer = optim.SGD(parameters_to_optimize, lr=lr, \n","                            momentum=curr_momentum, \n","                            weight_decay=weight_decay)\n","  scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=curr_gamma)\n","\n","  # lists that accumulate loss/accuracy values over the training period\n","  train_loss = []\n","  train_acc = []\n","  test_loss = []\n","  test_acc = []\n","\n","  running_corrects = 0\n","  tot_samples = 0\n","  for i in range(epochs):\n","    counter_mod = 0\n","    n_iters = 0\n","    train_main_acc_val = 0\n","    train_main_loss_val = 0\n","\n","    train_zoom_source_loss_val = 0\n","\n","    train_zoom_target_loss_val = 0\n","    \n","    test_main_acc_val = 0\n","    test_main_loss_val = 0\n","    \n","    net.train()\n","    for source_batch, target_batch, source_rot_batch, target_rot_batch in zip(source, target, source_rot, target_rot):\n","\n","      S, _ = format_batch(source_batch, pretext_task=\"zoom\")\n","      T, _ = format_batch(target_batch, pretext_task=\"zoom\")\n","\n","      _, S_hat = format_batch(source_rot_batch, pretext_task=\"zoom\")\n","      _, T_hat = format_batch(target_rot_batch, pretext_task=\"zoom\")\n","\n","      # zero the gradients\n","      optimizer.zero_grad() \n","\n","      # MAIN TASK\n","      # setup SOURCE DOMAIN STANDARD dataset to feed to the net\n","      source_rgb_images = S[\"rgb\"].to(DEVICE)\n","      source_depth_images = S[\"depth\"].to(DEVICE)\n","      source_main_labels = S[\"label\"].to(DEVICE)\n","      #train on source zoomed \n","      outputs = net.forward(source_rgb_images, source_depth_images, mode=\"main\")\n","      loss_M = main_criterion(outputs, source_main_labels)\n","      #compute stats\n","      _, preds = torch.max(outputs.data, 1)\n","      running_corrects = torch.sum(preds == source_main_labels.data).data.item()\n","      tot_samples = len(source_main_labels)\n","      train_main_loss_val += loss_M.item()\n","      train_main_acc_val += running_corrects/tot_samples\n","\n","      \n","      # entropy minimization\n","      # setup TARGET DOMAIN STANDARD dataset to feed to the net\n","      target_rgb_images = T[\"rgb\"].to(DEVICE)\n","      target_depth_images = T[\"depth\"].to(DEVICE)\n","      # target labels in this phase can't be used for training\n","      # train on source original images \n","      outputs = net.forward(target_rgb_images, target_depth_images, mode=\"main\")\n","      loss_entropy_min = entropy_min_criterion(outputs)\n","          \n","\n","      # PRETEXT TASK \n","      #setup  SOURCE DOMAIN ZOOMED dataset to feed to the net\n","      source_zoom_rgb_images = S_hat[\"rgb\"].to(DEVICE)\n","      source_zoom_depth_images = S_hat[\"depth\"].to(DEVICE)\n","      source_zoom_labels = S_hat[\"label\"].to(DEVICE)\n","      #train on source zoomed \n","      outputs = net.forward(source_zoom_rgb_images, source_zoom_depth_images, mode=\"pretext\")\n","      loss_P_1 = pretext_criterion(outputs.flatten(), source_zoom_labels) \n","      #compute stats\n","      train_zoom_source_loss_val += loss_P_1.item()\n","\n","      #setup TARGET DOMAIN ZOOMED dataset to feed to the net\n","      target_zoom_rgb_images = T_hat[\"rgb\"].to(DEVICE)\n","      target_zoom_rgb_depth_images = T_hat[\"depth\"].to(DEVICE)\n","      target_zoom_labels = T_hat[\"label\"].to(DEVICE)\n","      # train on target zoomed\n","      outputs = net.forward(target_zoom_rgb_images, target_zoom_rgb_depth_images, mode=\"pretext\")\n","      loss_P_2 = pretext_criterion(outputs.flatten(), target_zoom_labels)\n","      #compute stats\n","      train_zoom_target_loss_val += loss_P_2.item()\n","      \n","\n","\n","\n","      # BACKPROP WITH THE FULL LOSS\n","      loss = loss_M + (em_weight/tot_samples)*loss_entropy_min + lambda_*(loss_P_1 + loss_P_2)\n","\n","      loss.backward() \n","      # UPDATE GRADIENTS\n","      optimizer.step()\n","\n","      # accumulate gradients and update them once every two batches\n","      counter_mod += 1\n","      n_iters += 1\n","    \n","    train_acc.append(train_main_acc_val/n_iters)\n","    train_loss.append(train_main_loss_val/n_iters)\n","\n","    print(\"EPOCH \", i + 1)\n","    print(\"train main accuracy: \", train_main_acc_val/n_iters)\n","    print(\"train main loss: \", train_main_loss_val/n_iters)\n","    print(\"train zoom source loss: \", train_zoom_source_loss_val/n_iters)\n","    print(\"train zoom target loss: \", train_zoom_target_loss_val/n_iters)\n","\n","\n","    # TEST RESULTS OF THE CURRENT BATCH OF TRAINING\n","    if not light_validation or i == epochs - 1:\n","      net.eval() \n","      n_iters = 0\n","      for target_batch in target: \n","        \n","        # Format batch\n","        T, _ = format_batch(target_batch, pretext_task=\"zoom\") \n","      \n","        # prepare target data\n","        target_rgb_images = T[\"rgb\"].to(DEVICE)\n","        target_depth_images = T[\"depth\"].to(DEVICE)\n","        target_main_labels = T[\"label\"].to(DEVICE)\n","      \n","        outputs = net.forward(target_rgb_images, target_depth_images, mode=\"main\")\n","        loss_T = main_criterion(outputs, target_main_labels)\n","\n","        # compute test stats\n","        _, preds = torch.max(outputs.data, 1)\n","        running_corrects = torch.sum(preds == target_main_labels.data).data.item()\n","        test_main_acc_val += running_corrects/len(target_main_labels)\n","        test_main_loss_val += loss_T.item()\n","  \n","        n_iters += 1\n","\n","      test_loss.append(test_main_loss_val/n_iters)\n","      test_acc.append(test_main_acc_val/n_iters)\n","\n","      print(\"test main target accuracy: \", test_main_acc_val/n_iters)\n","      print(\"test main target loss: \", test_main_loss_val/n_iters)\n","    \n","    print()\n","    scheduler.step()\n","  \n","  return net, train_loss, train_acc, test_loss, test_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"MuN6h_aDSurr"},"source":["# Datasets"]},{"cell_type":"markdown","metadata":{"id":"mK9OhQxJLuoZ","colab_type":"text"},"source":["## ROD and synROD - rotation"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zuh17u3HL5ZF","colab":{}},"source":["tfConfig = TransformConfig(resize_shape=256, centercrop_shape=224)   # or resize_shape=\n","# config types are imagenet, rgb_mod, depth_mod, rgb_depth_mod                              \n","synrod_param_values, rod_param_values = tfConfig.get_rotation_configuration(config_type=\"imagenet\")    # mod corresponds to the modification of imagenet weights with the computed ones"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Y8soSqM1L5Zd","colab":{}},"source":["\n","synrod = SynROD(synrod_path,\n","                item_extractor_fn=\"rotation\",\n","                item_extractor_param_values= synrod_param_values,\n","                 ram_mode=False)\n","rod = ROD(rod_path,\n","                item_extractor_fn=\"rotation\",\n","                item_extractor_param_values=rod_param_values,\n","                 ram_mode=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zn_LHa-qMmpu","colab_type":"text"},"source":["## ROD and synROD - zoom"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"20esK9E7MxKh","colab":{}},"source":["tfConfig = TransformConfig(resize_shape=256, centercrop_shape=224)   # or resize_shape=\n","# config types are imagenet, rgb_mod, depth_mod, rgb_depth_mod                              \n","synrod_param_values, rod_param_values = tfConfig.get_zoom_configuration(config_type=\"imagenet\")    # mod corresponds to the modification of imagenet weights with the computed ones"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Rp7A3ZsCMxKu","colab":{}},"source":["\n","synrod = SynROD(synrod_path,\n","                item_extractor_fn=\"zoom\",\n","                item_extractor_param_values= synrod_param_values,\n","                 ram_mode=False)\n","rod = ROD(rod_path,\n","                item_extractor_fn=\"zoom\",\n","                item_extractor_param_values=rod_param_values,\n","                 ram_mode=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"oOEEOG89J-kG"},"source":["## ROD and synROD - decentralized zoom"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Cq3eM5TwJ-kK","colab":{}},"source":["tfConfig = TransformConfig(resize_shape=256, centercrop_shape=224)   # or resize_shape=\n","# config types are imagenet, rgb_mod, depth_mod, rgb_depth_mod                              \n","synrod_param_values, rod_param_values = tfConfig.get_zoom_configuration(config_type=\"imagenet\")    # mod corresponds to the modification of imagenet weights with the computed ones"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bmR5CxHkJ-kU","colab":{}},"source":["\n","synrod = SynROD(synrod_path,\n","                item_extractor_fn=\"decentralized_zoom\",\n","                item_extractor_param_values= synrod_param_values,\n","                 ram_mode=False)\n","rod = ROD(rod_path,\n","                item_extractor_fn=\"decentralized_zoom\",\n","                item_extractor_param_values=rod_param_values,\n","                 ram_mode=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fi4VCiGGFkS8","colab_type":"text"},"source":["# Tuning process"]},{"cell_type":"code","metadata":{"id":"zZK3XCEa136C","colab_type":"code","colab":{}},"source":["#add/remove parameters of choice\n","parameters_dict = { \"lr\" : [0.0001, 0.0005, 0.001, 0.005, 0.01],\n","                    \"batch_size\":[16,32,64, 128],\n","                   \"epochs\":[10, 20, 30],\n","                   \"weight_decay\":[1e-1, 1e-2, 1e-3, 1e-4, 1e-5],\n","                    \"step_size\": [30, 60],\n","                   }\n","\n","paramGrid = ParameterGrid(parameters_dict)\n","paramgrid_list = [grid_val for grid_val in paramGrid]\n","\n","log_file = \"test_tuning.csv\"\n","SAVE_PATH = \"/content/drive/My Drive/DL_project/logs/\" + log_file\n","\n","# Create or overwrite file with heading\n","with open(SAVE_PATH, \"w\") as f:\n","  header = ['lr', 'batch_size', 'epochs', 'weight_decay', 'step_size', \"train_accuracy\", \"train_loss\", \"validation_accuracy\", \"validation_loss\"]\n","  f.write(\", \".join(header) + '\\n')\n","  f.close()\n","\n","# Now open in append mode\n","f = open(SAVE_PATH, \"a\")\n","\n","for grid in paramgrid_list:\n","  print(\"CONFIG:\")\n","  print(grid)\n","\n","  # Train net\n","  net, train_loss, train_acc, test_loss, test_acc = train_net(synrod, rod, grid, type=\"rgb\", light_validation=True)\n","  \n","  if len(test_loss) > 1: # Not in light_validation mode\n","    learning_curves(train_acc, train_loss, test_acc, test_loss, \"Example of learning curves\")\n","    print()\n","\n","  # Append on file\n","  config_data = [grid['lr'], grid['batch_size'], grid['epochs'], grid['weight_decay'], grid['step_size'],\n","                 train_acc[-1], train_loss[-1], test_acc[-1], test_loss[-1]]\n","  f.write(\", \".join([str(el) for el in config_data]) + '\\n')\n","  f.flush()\n","\n","f.close()"],"execution_count":0,"outputs":[]}]}